---
title: Consent Decree and Arrest Rates
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Issues

+ Some code chunks contain horizontal scroll bars. How can we improve the code output for readability? 

+ Warning is appearing for some code. Why? How should we address this?

+ Historical plot (646-660) needs to be more elegant. 
    + Should we include this in the case study? 
+ Should the analysis dataframe be appended as our analysis continues or should it be finalized in preparing for our analysis?
+ What are some good resources for explaining chi square goodness of fit test? 
+ When running a model with all - 1 covariates I get a warning message lgm.fit :algorithm did not converge.
    + What does this mean? 
+ NA values in glm model output 
+ How to check for multicollinearity given dummy variables

```{r}
seed <- sum(c(which(letters=='o'), which(letters=='c'), which(letters=='s')))
set.seed(seed)
```

# Motivation 

Following the death of Freddie Gray on April 12^th^, 2015, Mayor Stephanie Rawlings-Blake formally requested that an investigation of Baltimore's policing practices be taken by the Department of Justice (DoJ). The DoJ, after a yearlong investigation, found that the Baltimore Police Department (BPD) took part in unconstitutional policing practices. A consent decree between BPD and the DoJ, which mandates a series of restrictions on officers, amongst other things, was approved by a judge. *LRJ: can we add a date for the approval and a date for when it would go into effect?  Can we link to an article that summarizes the significance of Freddie Gray's death?*

The goal of the consent decree is to ensure that the Baltimore Police Department resolve the unconstitutional policing practices identified by the DoJ investigation. Although court enforceable, the practical impact of the consent decree is still unknown, largely because the scope of its various components and the underlying complexity of each of those components.

An excerpt from page 48 of [the DoJ's report](https://www.justice.gov/crt/file/883296/download) highlights the multiple infractions suspected of the Baltimore Police Department: 

---

![](DoJ_Excerpt.png)

---


In this case study, we will look at patterns in arrests by BPD over time with the goal of assessing whether the consent decree had an impact on arrest patterns. 

The learning objectives include data wrangling, data visualization, regular expressions, and Poisson regression. 

The libraries used in this case study will be:

+ `tidyverse`, for data wranging and summarization, amongst other things, from a number of packages

+ `ggplot2`, for creating beautiful visualizations with ready data

+ `lubridate`, for converting time data into a form that can be used

+ `MASS`, for conducting a negative binomial regression

+ `stringr`, for handling regular expression

+ `kableExtra`, for creating beautiful HTML tables

+ `gridExtra`, for creating plots of plots

# What is the data? 

The analysis uses data from two sources:

+ [Open Baltimore](https://data.baltimorecity.gov/)

+ [American Community Survey](https://www.census.gov/programs-surveys/acs)

Open Baltimore is a web portal with Baltimore specific data managed by the City of Baltimore. All arrest data from the Baltimore Police Department used in this analysis is made available on Open Baltimore [here](https://data.baltimorecity.gov/Public-Safety/BPD-Arrests/3i3v-ibrt).

The American Community Survey is an ongoing survey by the US Census Bureau that captures demographic information for the United States and its regions. We will be using age, sex, and race data (tables B01001A and B01001B) for the county "Baltimore city, Maryland." This data is made available by the US Census Bureau [here](https://www.census.gov/data.html). 

# Data import {#data-import}

We start by loading the libraries we will need for this case study.

```{r, warning=FALSE, message=FALSE}
library(MASS)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(stringr)
library(kableExtra)
```

**LRJ: I would separate the import of the two datasets.  First import and talk about the arrest data.  Then do the same with the population data after talking about how we need population data in order to work with arrest rates rather than counts of arrests.  In addition to talking about the dimension of each data frame, say a little bit (be brief) about what variables are in each.**

**DESCRIBE EACH DATA SET AFTER LOADING IT** 

We import arrest data, as well as census data by race. These files were downloaded from [American FactFinder](https://factfinder.census.gov/faces/nav/jsf/pages/guided_search.xhtml) and [Open Baltimore](https://data.baltimorecity.gov/Public-Safety/BPD-Arrests/3i3v-ibrt).

**LRJ: we want to read in with the read_csv() function.  we may have to fix some variable names after doing this**

```{r}
BPD_Arrests_March1 <- read_csv("BPD_Arrests_March1.csv")
```

Notice the number of parsing failures. `read_csv()` predicts the class of each column when loading data. Sometimes these predictions are difficult for `read_csv` to make when the data has errors. Sometimes these predictions are incorrect.

Still, these predictions and errors are useful. With `read_csv()`, we can explicitly assign data classes. Having predictions beforehand allows us to see our data and identify potential data collection errors. 

The `problems()` function from `read_csv` allows us to retrieve the problems see in our initial `read_csv` output. We can determine which columns were causing problems by converting the `tibble` created by `read_csv()` into a dataframe using `as.data.frame`, converting the `col` column to a format `as.factor()` can comprehend, and then using the `summary()` function to create a count of parsing problems caused by each column in our original dataframe. 

```{r}
summary(as.factor(as.data.frame(problems(BPD_Arrests_March1))[,"col"]))
```

It looks like our coordinate data is causing problems.

If we look at the actual output created by the `problems()` function, we see that the there is some `character` class data where `double`/`numeric` class data should be. 

```{r}
problems(BPD_Arrests_March1)
```

Let's look the columns we will be working with.

```{r}
colnames(BPD_Arrests_March1)
```

As you can see, we have some demographic information about the person arrested. We also have some information about the nature of the arrest. Spatial data is also available. 

The object containing our data is currently a `tibble`. The `tibble` class is great for printing data neatly, but in most cases does not provide substantial advantages over the `dataframe` class. To get a peak at our data without any of the special formatting provided by the `tibble` class, we can use the functions `as.data.frame()` and `head()` to print all columns of data for the first several rows.  

```{r}
head(as.data.frame(BPD_Arrests_March1))
```

Now that we know what data we are working with, let's specify the classes of our columns with `read_csv()`. 

We have the option to not read in some of the data while we are specifying the classes with `read_csv()`. This can be done with the `cols_only()` function, which specifies which columns to read in. 

If we run the code below, we get a parsing failure for each row with a date.

```{r}
BPD_Arrests_March1 <- read_csv("BPD_Arrests_March1.csv", col_types = cols_only(Age=col_character(),
                                                                          Sex=col_character(),
                                                                          Race=col_character(),
                                                                          ArrestDate=col_date()))
```

This is because `read_csv` cannot read in our date data in its current form. 

For now, let's specify all of the columns we will be using as a `character class`.

Using the `character` class for data allows us to convert columns of our data to the appropriate class afterward without any formatting issues. This is particularly useful when data is not in a form that read_csv can use to specify a particular class like in the case above.

Let's read in the data using the more conservative approach of specifying the columns we need as a `character` class. 

```{r}
BPD_Arrests_March1 <- read_csv("BPD_Arrests_March1.csv", col_types = cols_only(Age=col_integer(),
                                                                          Sex=col_character(),
                                                                          Race=col_character(),
                                                                          ArrestDate=col_character()))
```

Let's check if our `tibble` has any problems.

```{r}
length(as.factor(as.data.frame(problems(BPD_Arrests_March1))[,"col"]))
```

```{r}
BaltimoreWhitePop <- read.csv("ACS_17_5YR_B01001A_W.csv", header=FALSE) #Update this to reflect download name of csv from actual source.
```


```{r}
BaltimoreBlackPop <- read.csv("ACS_17_5YR_B01001B_BAA.csv", header=FALSE) #Update this to reflect download name of csv from actual source.
```

```{r, eval=FALSE}
View(BaltimoreBlackPop)
```

![](View()_3.png)

We then look at the dimensions of each dataframe to get a handle on how large and complicated the dataframes are. 

The dataframe for arrests in Baltimore has `r dim(BPD_Arrests_March1)[1]` rows and `r dim(BPD_Arrests_March1)[2]` columns.

```{r}
dim(BPD_Arrests_March1)
```

The dataframe for the White population in Baltimore has `r dim(BaltimoreWhitePop)[1]` rows and `r dim(BaltimoreWhitePop)[2]` columns.

```{r}
dim(BaltimoreWhitePop)
```

The dataframe for the Black population in Baltimore has `r dim(BaltimoreBlackPop)[1]` rows and `r dim(BaltimoreBlackPop)[2]` columns.

```{r}
dim(BaltimoreBlackPop)
```

Now that we know a little bit about our dataframes, we can begin the process of transforming them for our analysis.

# Data wrangling 

Since three dataframes will be used for this analysis, it is important that we identify the role of each before continuing.

Simply put, we will be analyzing temporal trends in arrests while adjusting for the size of subpopulations. Our final dataframe should therefore have two components:

+ 1.0 Arrest data
    + 1.1 Open Baltimore Arrest Data
+ 2.0 Population data
    + 2.1 White population data
    + 2.2 Black population data

For our poisson regression, we need each of these components to be grouped in such a way that would allow them to be combined to make our final dataframe. 

Let us create a quick workflow.

**Workflow**

-----

We'll begin by making each of the three dataframes more practical to use. Then we'll combine the population datasets to make a population dataframe. Lastly, we'll create a final dataframe by grouping the arrest and population dataframes similarly and then combine the two.  

-----

**Making the arrest data more practical**

We'll begin with the arrest data.

**Consider removing this.**

We can get rid of the spatial data.

We need to be careful when doing this, however. We have the `MASS` and `tidyverse` package loaded. `dplyr`, a package within the `tidyverse`, has a function that we can use called `select()`. `MASS` also has a `select()` function. These functions, `dplyr::select()` and `MASS::select()`, although identically named, are different, meaning whichever package we load second will mask the identically named function of the package loaded first. Since we loaded `dplyr` second (by loading `tidyverse`), our `select()` function is that from `dplyr`. A sure way to use the specific function you intend to use is by specifying the package with a prefix of the form `package::function()`. We do this below, although it isn't necessary since `dplyr`(again, from `tidyverse`) was loaded second.
**Consider removing this.^**

We need to know what type of data we are working with to appropriately work with out data.

Rather than use the `class()` function for each individual variable, we can use `lapply()` to apply the `class()` function to all of our columns. This will save us `r length(colnames(BPD_Arrests_March1))-1` lines of code.

```{r}
lapply(BPD_Arrests_March1[,1:length(colnames(BPD_Arrests_March1))],class)
```

It looks like none of our variables are not specified correctly. This makes sense since we read the columns in as character classes. 

Using `as.Date`, let's make sure R recognizes that the data is in month-day-year format.

```{r}
BPD_Arrests_March1$ArrestDate <- as.Date(BPD_Arrests_March1$ArrestDate, "%m/%d/%y")
```

Now we do the same thing for our other variables, using the appropriate functions to designate appropriate classes for our data.

```{r}
BPD_Arrests_March1$Age <- as.integer(BPD_Arrests_March1$Age)
BPD_Arrests_March1$Sex <- as.factor(BPD_Arrests_March1$Sex)
BPD_Arrests_March1$Race <- as.factor(BPD_Arrests_March1$Race)
```

We need to know a bit more about the values in each column before going forward with assigning levels to our factors class columns.

```{r}
lapply(BPD_Arrests_March1[,1:length(colnames(BPD_Arrests_March1))],summary)
```

We are now ready to assign levels to our factor class columns. For this analysis, we will only use arrest and population data for the black and white populations in Baltimore. Our dataset still contains information for arrests of people of other races. We need to exclude this data. 

```{r}
BPD_Arrests_March1$Sex <- factor(BPD_Arrests_March1$Sex, levels = c("F", "M"), labels=c("Female", "Male"))
BPD_Arrests_March1 <- BPD_Arrests_March1[BPD_Arrests_March1$Race=="B"|BPD_Arrests_March1$Race=="W",]
BPD_Arrests_March1$Race <- factor(BPD_Arrests_March1$Race, levels = c("B", "W"), labels = c("Black", "White"))
```

Let's look at our dataframe again.

```{r}
head(BPD_Arrests_March1)
```

We'll group this final dataframe once we know the limitations of our population data. 

**Making the population data more practical**

We'll begin with the white population data.

![](View()_2.png)

Sometimes imported dataframes follow a rigorous codified format. This is more than often the case with very well maintained data. Codified dataframes often require a corresponding codebook.

We imported the dataframes without designating headers for this very reason. Census dataframes are often codified and contain a second set of headers for practical use. Although making the dataframe more functional is still possible without this step, importing the data in this way saves us a few steps.

Our arrest data has practical field names and no second set of headers. Our census data has two sets of headers. 

Our first glance at the census data (see [Data Import](#data-import)) revealed that the first rows of the census data we imported are codified while the second rows are practically named.

We will use the second rows of the census data as field names. This will allow different fields to be easily discernable from one another. 

This can be done simply by treating the second row as a list comprised of character class items. 

```{r}
colnames(BaltimoreWhitePop) <- as.character(BaltimoreWhitePop[2,])
colnames(BaltimoreWhitePop[,1:5])
```

Our colnames are now a set of numbers. This is because *EXPLANATION*. *EXPLAIN WHY USING THESE COLNAMES WOULD BE A TERRIBLE IDEA* **ALSO: why are the colnames numbers to begin with?**

We cannot take an object that is a non-matrix-like object and the `colnames()` functions together. `colnames()` only recognizes matrix-like objects (see `?colnames()`.

We can transform the second row of the dataframe using the `unlist()` function, which produces a vector (a matrix-like object) from an object. 

```{r}
colnames(BaltimoreWhitePop) <- as.character(unlist(BaltimoreWhitePop[2,]))
colnames(BaltimoreWhitePop[,1:5])
```

Now that we have appropriate headers, let us look at our dataframes.

```{r}
head(BaltimoreWhitePop[,1:5])
```

For each column, there is a field name, the original codified row, and the practical row we used to designate our field names. We can remove these now that we have appropriate field names

```{r}
BaltimoreWhitePop <- BaltimoreWhitePop[-(1:2),]
```

The census dataframes we imported are grouped by age group and gender. Each estimate has a corresponding margin of error. For this analysis, we will only be using the provided estimates. 

For efficiency, we should discard any data that will not be used in our analysis. We could manually remove one column at a time with multiple lines of code. This would take some time and and may involve some trial and error. 

Instead, we will use `substr` to take our character class field names and identify the fields we will need. 

This takes advantage of a programming concept known as [regular expressions](https://en.wikipedia.org/wiki/Regular_expression?oldformat=true). Simply put, a sequence of text in the data. 

`substr` searches for a distinct character string, a regular expressions, within a portion of the characters in a string. Below, we search for `Estimate` within the 1^st^ through 8^th^ characters in each field name and then save the columns that meet that criteria. We also save and rename the `Geography` column as good practice. Rownames are removed to avoid later confusion. 

```{r}
Geography <- BaltimoreWhitePop$Geography
BaltimoreWhitePop <- cbind(Geography,
        BaltimoreWhitePop[, substr(colnames(BaltimoreWhitePop),1,8)=="Estimate"])
rownames(BaltimoreWhitePop) <- c()
head(BaltimoreWhitePop[,1:5])
```

Let us look at our new column names.

```{r, echo=FALSE}
colnames(BaltimoreWhitePop)
```

`"Estimate; "` is in nearly every column name in our new dataframe. Let's remove it to have an even more practical dataframe. To do this, we will use `str_remove` from the `stringr` package.

```{r}
colnames(BaltimoreWhitePop) <- str_remove(colnames(BaltimoreWhitePop),"Estimate; ")
```

Our new column names are much simpler.

```{r}
colnames(BaltimoreWhitePop)
```

Let's remove symbols, spaces, and `years` from our field names. The expression `[[:punct:]]` encompasses a wide range of symbols. `str_replace_all` differs from `str_replace` in that it is used to replace patterns for the entire string. A number of resources exist online showing the ways we can use regular expressions (click [here](http://web.mit.edu/hackl/www/lab/turkshop/slides/regex-cheatsheet.pdf) to see this great MIT resource).

```{r}
colnames(BaltimoreWhitePop) <- str_replace_all(colnames(BaltimoreWhitePop), "[[:punct:]]", "")
colnames(BaltimoreWhitePop) <- str_replace_all(colnames(BaltimoreWhitePop), " ", "")
colnames(BaltimoreWhitePop) <- str_replace_all(colnames(BaltimoreWhitePop), "years", "")
```

Now our column names are practical and concise.

```{r}
colnames(BaltimoreWhitePop)
```

We need to organize our dataframe to have a single column for age group and a single column gender. This data format is known as *long format* and is required to efficiently create data visualizations with the `ggplot` package. Doing this will also allow us to use more appropriate variables such as age group and gender as well as the specific age group-gender combinations the dataframe currently provides. 

Our dataframe is still a bit messy. It is currently in wide format has two variables of interest in nearly every column. We need to get our dataframe into long format and remove the variable pairings. To do this we will have to collapse the Population data gathered across columns into a single column current combinations and use this column to create a dataframe with age group and gender columns.

*WHY THE WARNING?*

```{r}
molten.BaltimoreWhitePop <- BaltimoreWhitePop %>% gather(key=AgeGroupGender, value=Population, -Geography)
head(molten.BaltimoreWhitePop)
```

Let us remove rows with from `variable` representing totals for each variable. This will simplify the summaries we create later with `dplyr`.

```{r}
molten.BaltimoreWhitePop <- molten.BaltimoreWhitePop[molten.BaltimoreWhitePop$AgeGroupGender!="Total",]
molten.BaltimoreWhitePop <- molten.BaltimoreWhitePop[molten.BaltimoreWhitePop$AgeGroupGender!="Male",]
molten.BaltimoreWhitePop <- molten.BaltimoreWhitePop[molten.BaltimoreWhitePop$AgeGroupGender!="Female",]
```

Now, let us create two empty columns, `AgeGroup` and `Gender`.

```{r}
molten.BaltimoreWhitePop$AgeGroup <- NA
molten.BaltimoreWhitePop$Gender <- NA
head(molten.BaltimoreWhitePop)
```

Let us now fill our `AgeGroup` and `Gender` columns with information from our `variable` column. 

To do this we will we will identify patterns within strings and use those specific pattern to fill up our `AgeGroup` and `Gender` columns. This will involve using the `substr()` function along with an ifelse() statement for each of our columns. We will check whether our ifelse() functions worked by combining the `sum()` and `is.na()` functions.

```{r}
molten.BaltimoreWhitePop$AgeGroupGender <- as.character(molten.BaltimoreWhitePop$AgeGroupGender)
molten.BaltimoreWhitePop$Gender <- ifelse(substr(molten.BaltimoreWhitePop$AgeGroupGender,1,4)=="Male","Male",
ifelse(substr(molten.BaltimoreWhitePop$AgeGroupGender,1,6)=="Female","Female",NA
       )
)
molten.BaltimoreWhitePop$AgeGroup <- ifelse(substr(molten.BaltimoreWhitePop$AgeGroupGender,1,4)=="Male",str_replace_all(molten.BaltimoreWhitePop$AgeGroupGender,"Male",""),
                                            ifelse(substr(molten.BaltimoreWhitePop$AgeGroupGender,1,6)=="Female",str_replace_all(molten.BaltimoreWhitePop$AgeGroupGender,"Female",""),NA)
                                            )
head(molten.BaltimoreWhitePop)
```

We can get rid of extraneous fields now that we have the fields we need.

```{r}
molten.BaltimoreWhitePop$AgeGroupGender <- NULL
```

Recall leaving the `Geography` column as good practice. Leaving columns specific to a dataframe allow the dataframe to be easily identified from other dataframes after merging. We will be merging this dataframe to the other census dataframe to create a new dataframe with the population demographics we will be adjusting for in our analysis. To help identify which race this dataframe is specific to, we will add a new column, `Race`, with an appropriate factor.

```{r}
molten.BaltimoreWhitePop$Race <- as.factor("White")
```

Our `Population` variable should be last in our dataframe. `dplyr` could be helpful for this purpose. Let's select/subset the entire dataframe and conveniently reorder our columns. Notice that quotation marks are not necessary after piping with `dplyr`.

```{r}
molten.BaltimoreWhitePop <- molten.BaltimoreWhitePop %>% arrange(Geography,AgeGroup,Gender,Race,Population)
head(molten.BaltimoreWhitePop)
```

Let us take a quick look at our progress thus far.

```{r}
head(molten.BaltimoreWhitePop)
```

If our two dataframes are identically formatted, the data wrangling steps we have made up to this point can be repeated to yield a similar dataframe without error.

```{r}
colnames(BaltimoreBlackPop) <- as.character(unlist(BaltimoreBlackPop[2,]))
BaltimoreBlackPop <- BaltimoreBlackPop[-(1:2),]
Geography <- BaltimoreBlackPop$Geography
BaltimoreBlackPop <- cbind(Geography,
        BaltimoreBlackPop[, substr(colnames(BaltimoreBlackPop),1,8)=="Estimate"])
rownames(BaltimoreBlackPop) <- c()
colnames(BaltimoreBlackPop) <- str_remove(colnames(BaltimoreBlackPop),"Estimate; ")
colnames(BaltimoreBlackPop) <- str_replace_all(colnames(BaltimoreBlackPop), "[[:punct:]]", "")
colnames(BaltimoreBlackPop) <- str_replace_all(colnames(BaltimoreBlackPop), " ", "")
colnames(BaltimoreBlackPop) <- str_replace_all(colnames(BaltimoreBlackPop), "years", "")
molten.BaltimoreBlackPop <- BaltimoreBlackPop %>% gather(key=AgeGroupGender, value=Population, -Geography)
molten.BaltimoreBlackPop <- molten.BaltimoreBlackPop[molten.BaltimoreBlackPop$AgeGroupGender!="Total",]
molten.BaltimoreBlackPop <- molten.BaltimoreBlackPop[molten.BaltimoreBlackPop$AgeGroupGender!="Male",]
molten.BaltimoreBlackPop <- molten.BaltimoreBlackPop[molten.BaltimoreBlackPop$AgeGroupGender!="Female",]
molten.BaltimoreBlackPop$AgeGroup <- NA
molten.BaltimoreBlackPop$Gender <- NA
molten.BaltimoreBlackPop$AgeGroupGender <- as.character(molten.BaltimoreBlackPop$AgeGroupGender)
molten.BaltimoreBlackPop$Gender <- ifelse(substr(molten.BaltimoreBlackPop$AgeGroupGender,1,4)=="Male","Male",
ifelse(substr(molten.BaltimoreBlackPop$AgeGroupGender,1,6)=="Female","Female",NA
       )
)
molten.BaltimoreBlackPop$AgeGroup <- ifelse(substr(molten.BaltimoreBlackPop$AgeGroupGender,1,4)=="Male",str_replace_all(molten.BaltimoreBlackPop$AgeGroupGender,"Male",""),
                                            ifelse(substr(molten.BaltimoreBlackPop$AgeGroupGender,1,6)=="Female",str_replace_all(molten.BaltimoreBlackPop$AgeGroupGender,"Female",""),NA)
                                            )
molten.BaltimoreBlackPop$AgeGroupGender <- NULL
molten.BaltimoreBlackPop$Race <- as.factor("Black")
molten.BaltimoreBlackPop <- molten.BaltimoreBlackPop %>% 
arrange(Geography,AgeGroup,Gender,Race,Population)
head(molten.BaltimoreBlackPop)
```

As you can see, after repeating the same data wrangling steps, the second dataframe is similarly formatted. 

Let's combine demographic data from the two to create a dataframe from which to base our offset for our regression analysis. We will use `bind_rows` from the `dplyr` package. 

```{r}
demographics <- rbind(molten.BaltimoreWhitePop,molten.BaltimoreBlackPop)
head(demographics)
```

Oops! We did some serious wrangling and have not checked our column classes. Let's save some time and apply the `class()` function to all columns of our new dataframe to see whether we inadvertently made some changes along the way that can complicate further wrangling. 

```{r}
lapply(demographics[,1:length(colnames(demographics))],class)
```

Although we set our `Race` columns as factors prior to using `rbind()`, the `Race`column in the new dataframe is a character class. Furthermore, our `Gender`, `AgeGroup`, and `Population` columns are incorrectly specificed as `character` classes.

We are about to use `dplyr` and `ggplot` for exploratory data analysis. Now would be a good time to fix this. 

```{r}
demographics[,c("Gender","AgeGroup","Race")] <- lapply(demographics[,c("Gender","AgeGroup","Race")],as.factor)

demographics$Population <- as.numeric(demographics$Population)
lapply(demographics[,1:length(colnames(demographics))],class)
```

The `head()` function only returns several of the uppermost rows, meaning that we are not be able to assess what happened to our rows after we used the `bind_rows()` function. We loaded the `dplyr` package for data summarization. If our dataframes were bound and specified correctly, we should be able to accurately recreate summaries of the data. Let's use the `group_by`,`summarise()`, and `mutate()` functions to do just that. In 2017, Baltimore was roughly 30% White and 63% Black (see the [US Census Estimates for Baltimore City](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B02001&prodType=table)). If we bound and specified our dataframes correctly, we should be able to generate similar figures.

In 2017, about 93% of Baltimore was either White or Black. Let us use that information and compare it to the [US Census Estimates for Baltimore City](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_17_5YR_B02001&prodType=table).

```{r}
TotalPopulationMultiplier <- (100/93)
demographics %>%
  group_by(Race) %>% summarise(n = sum(Population)) %>% mutate(Percent = n/(sum(n)*(TotalPopulationMultiplier)))
```

Our figures are consistent with those by the census. 

We need to group the arrest and population data similarly so that we can merge information from both dataframes into a single dataframe for analysis. 

To group our dataframes properly, we need to know the ways they are similar and the ways they are different. 

```{r}
colnames(demographics)
colnames(BPD_Arrests_March1)
```

Our dataframes are similar in the following ways: 

+ Both have information by sex or gender. **Insert disclaimer about police department using sex and not gender**

+ Both have information by race. 

+ Both have information pertaining to age. 

Our dataframes are different in the following ways: 

+ Our demographic data defines age by categorical groups and not in discrete values (as in the arrest data). 

+ Our demographic data does not have a temporal variable as does the arrest data.

+ Our demographic data has information for the Black and White population in Baltimore, whereas our arrest data includes arrests of people of other races. 

Before grouping, we need to define age similarly for both dataframes. We need to do this while preserving the temporal information provided by the arrest data. 

Now let's recode the age information provided by the arrest data by the age groups used in the demographic data. 

```{r}
levels(demographics$AgeGroup)
demographics <- demographics[demographics$AgeGroup!="Under5",]
demographics <- demographics[demographics$AgeGroup!="5to9",]
demographics <- demographics[demographics$AgeGroup!="10to14",]
demographics <- demographics[demographics$AgeGroup!="15to17",]
demographics$AgeGroup <- factor(demographics$AgeGroup, levels = c("18and19",
                                                "20to24",
                                                "25to29",
                                                "30to34",
                                                "35to44",
                                                "45to54",
                                                "55to64",
                                                "65to74",
                                                "75to84",
                                                "85andover"),
                       ordered = TRUE)
lapply(demographics, levels)
```

How do our levels compare?

#BOOKMARK

```{r}
lapply(BPD_Arrests_March1, levels)
```

```{r}
BPD_Arrests_March1 <- BPD_Arrests_March1[BPD_Arrests_March1$Age>=18,]
BPD_Arrests_March1 <- BPD_Arrests_March1[is.na(BPD_Arrests_March1$Age)==FALSE,]
BPD_Arrests_March1$Age <- cut(BPD_Arrests_March1$Age, breaks=c(17,19,24,29,34,44,54,64,74,84,Inf),labels=levels(demographics$AgeGroup),ordered_result = TRUE, include.lowest = FALSE)
summary(BPD_Arrests_March1$Age)
```

Let's change the names of the `Age` and `Sex` variables.

```{r}
colnames(BPD_Arrests_March1)[colnames(BPD_Arrests_March1)=="Age"] <- c("AgeGroup")
colnames(BPD_Arrests_March1)[colnames(BPD_Arrests_March1)=="Sex"] <- c("Gender")
colnames(BPD_Arrests_March1)
```

Now we can group and combine the data.

```{r}
ArrestData <- BPD_Arrests_March1 %>% group_by_all() %>% summarise(Arrests=n())
DemographicData <- demographics %>% group_by_at(vars(Gender,AgeGroup,Race,Population))
AnalysisDF <- merge(ArrestData, DemographicData, by=c("Gender","AgeGroup","Race"))
AnalysisDF$Month <- month(AnalysisDF$ArrestDate)
AnalysisDF$Year <- year(AnalysisDF$ArrestDate)
head(AnalysisDF)
```

Because we'll be analyzing temporal data, we should convert our temporal data into various units for analysis. 

```{r}
AnalysisDF <- AnalysisDF %>% mutate(ArrestDay = floor_date(ArrestDate,"day"),
                                    ArrestWeekday=weekdays(ArrestDate),
                                    ArrestWeek=floor_date(ArrestDate,"week"),
                                    ArrestMonth=floor_date(ArrestDate,"month"),
                          ArrestQuarter=floor_date(ArrestDate,"quarter"),
                          ArrestSeason=floor_date(ArrestDate,"season"),
                          ArrestYear=floor_date(ArrestDate,"year"))

AnalysisDF$ArrestWeekday <- factor(AnalysisDF$ArrestWeekday, levels = c("Sunday", "Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),labels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat"), ordered = TRUE)

head(AnalysisDF)
```

Let's explore the data! 

# Exploratory data analysis

Let's see if any related historical events coincided with changes in arrests.

Our data is currently grouped by date in `%m/%d/%y` format. Given the number of arrests in our dataset, it's likely that these arrests cover most if not all days in between 2014 and 2018. For visualization purposes, this may offer too high of a resolution to see any patterns that may exist. 

**Visit this [stackoverflow](https://stackoverflow.com/questions/18091721/align-geom-text-to-a-geom-vline-in-ggplot2) document to learn how to implement this more elegantly**

**alpha does not work on geom_point**

```{r, message=FALSE}
AnalysisDF %>% group_by(week=ArrestWeek) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x = week, y=n))+
  geom_point()+
  geom_smooth(color="red")+
  xlab('')+
  ylab('Arrests')+
  geom_vline(aes(xintercept=as.Date("2014-08-09"),color="1_Death of Michael Brown"), linetype="solid",size=3)+
  geom_vline(aes(xintercept=as.Date("2015-04-12"),color="2_Death of Freddie Gray"), linetype="solid",size=3)+ 
  geom_vline(aes(xintercept=as.Date("2015-05-06"),color="3_DoJ Investigation Requested by Major Blake"), linetype="solid",size=3) +
  geom_vline(aes(xintercept=as.Date("2016-08-10"),color="4_DoJ Announces Findings of Investigation"), linetype="solid",size=3) +
  geom_vline(aes(xintercept=as.Date("2017-04-07"),color="5_Consent Decree Approved"), linetype="solid",size=3)+
  scale_color_manual(name="Event",values=c("red","green","blue","orange","purple"),labels=c("Death of Michael Brown","Death of Freddie Gray","DoJ Investigation Requested by Major Blake","DoJ Announces Findings of Investigation","Consent Decree Approved"))+
  ggtitle("Arrests/Week in Baltimore")+
  theme(legend.position = "bottom", legend.direction = "vertical")
```

The deaths of Michael Brown and Freddie Gray both appear to immediately precede sharp declines in the number of arrests. The DoJ's announcement of its findings of its investigation also precedes a decline in the number of arrests. 

`geom_smooth` can often oversmooth our data, reducing its variance, meaning we may not be able to see any small scale variation that could be relevant for our analysis. 

We can use `gridExtra` to place all these plots in a single plot. This makes comparisons a bit easier. 

2019 has incomplete data, let's exclude it for now. 

```{r}
day <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(day=ArrestDay) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=day, y=n))+
  geom_line(color="red")
```

```{r}
week <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(week=ArrestWeek) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=week, y=n))+
  geom_line(color="purple")
```

```{r}
month <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(month=ArrestMonth) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=month, y=n))+
  geom_line(color="green")
```

```{r}
quarter <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(quarter=ArrestQuarter) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=quarter, y=n))+
  geom_line(color="pink")
```

```{r}
season <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(season=ArrestQuarter) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=season, y=n))+
  geom_line(color="blue")
```

```{r}
year <- AnalysisDF %>% filter(!(Year %in% 2019)) %>% group_by(year=ArrestYear) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=year, y=n))+
  geom_line(color="orange")
```

```{r}
grid.arrange(day,week,month,quarter,season,year,nrow=3,ncol=2,top="Plot 2")
```

It looks like using month units works well for observing large and small scale variation. We will use unit to explore the data some more temporally. Note that our decision to use this unit is strictly for vizualization purposes and does not necessarily mean that any specific pattern exists for statistical purposes. 

```{r}
AnalysisDF %>% group_by(ArrestMonth,Race,Gender,AgeGroup) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=ArrestMonth, y=n,color=AgeGroup))+
  geom_line(size=0.25,alpha=1)+
  facet_grid(Gender~Race)
```

Our vizualizations for representing Black or male arrests appear to highlight clear differences in arrest counts by age group. Let us now use rates.

```{r}
AnalysisDF %>% group_by(ArrestMonth,Race,Gender,AgeGroup) %>% mutate(r=sum(Arrests)/sum(Population)) %>% ggplot(aes(x=ArrestMonth, y=r,color=AgeGroup))+
  geom_line(size=0.25,alpha=1)+
  facet_grid(Gender~Race)
```

Note the two signals in one of the younger age groups for white females. This could either be indicative of short term variation we should be aware of of a small number problem. Recall that the small number problem occurs when the numerator for a rate is small and thus small changes in its count could result in very large changes in the corresponding rate. We could check to see if these signals are a result of a small number problem. 

```{r}
SignalDF <- AnalysisDF %>% filter(Year==2015:2017,Gender=="Female",Race=="White",AgeGroup=="20to24") %>% group_by(ArrestMonth,Race,Gender,AgeGroup) %>% mutate(r=Arrests/Population) %>% filter(r>=5e-04) %>% pull(Arrests)
unstable_arrest_counts <- unique(SignalDF) 
length(unstable_arrest_counts)
theoretical_arrests<- 1:10
theoretical_rate <- seq(1,10,by=1)/6744
theoretical_rate_by_arrests <- as.data.frame(cbind(theoretical_arrests,theoretical_rate))
theoretical_rate_by_arrests$theoretical_rate_ratio <- lag(theoretical_rate_by_arrests$theoretical_rate)/theoretical_rate_by_arrests$theoretical_rate
theoretical_rate_by_arrests$theoretical_rate_ratio_inverse <- 1/(lag(theoretical_rate_by_arrests$theoretical_rate)/theoretical_rate_by_arrests$theoretical_rate)
theoretical_rate_by_arrests <- theoretical_rate_by_arrests %>% gather(key=Type,value=Value,theoretical_rate_ratio,theoretical_rate_ratio_inverse,-theoretical_rate,-theoretical_arrests)
theoretical_rate_by_arrests <- theoretical_rate_by_arrests[complete.cases(theoretical_rate_by_arrests)==TRUE,]
ggplot(theoretical_rate_by_arrests,aes(x=theoretical_arrests, y=Value,color=Type))+
  geom_point(size=1)+
  geom_line(size=1)+
  geom_vline(xintercept=unstable_arrest_counts[1],color="black")
```

As you can see, the arrest counts are very low. It is likely that the signal found in the plot we made was just a result of unstable rates.

Weekdays cannot be linearly plotted, meaning we need to examine arrest counts by weekdays separately.

```{r}
AnalysisDF %>% group_by(ArrestWeekday,Race,Gender,AgeGroup) %>% mutate(n=sum(Arrests)) %>% ggplot(aes(x=ArrestWeekday,y=n,fill=AgeGroup))+
  geom_bar(stat="identity",position = 'dodge')+
  facet_grid(Gender~Race)
```

```{r}
AnalysisDF %>% group_by(ArrestWeekday,Race,Gender,AgeGroup) %>% mutate(r=Arrests/Population) %>% ggplot(aes(x=ArrestWeekday,y=r,fill=AgeGroup))+
  geom_bar(stat="identity",position = 'dodge')+
  facet_grid(Gender~Race)
```

Let's check our assumptions about the distribution of the outcome, arrest counts.

```{r}
arrests_n_per_day_poisson <- AnalysisDF %>% group_by(ArrestDay) %>% summarise(n = sum(Arrests))

arrests_n_per_day_nb <- AnalysisDF %>% group_by(ArrestDay) %>% summarise(n = sum(Arrests))

ggplot(data = arrests_n_per_day_poisson,
       mapping = aes(sample = n)) + stat_qq(distribution = stats::qpois, dparams = list(lambda = mean(arrests_n_per_day_poisson$n)),size=2.5, alpha=0.01,color="blue") + 
  stat_qq_line(distribution = stats::qpois, dparams = list(lambda = mean(arrests_n_per_day_poisson$n)),size=1, alpha=1, color="blue")+ 
  stat_qq(data = arrests_n_per_day_nb, distribution = stats::qnbinom, dparams = list(mu = mean(arrests_n_per_day_nb$n), size=length(arrests_n_per_day_nb$n)),size=2.5, alpha=0.01,color="red") + 
  stat_qq_line(data = arrests_n_per_day_nb, distribution = stats::qnbinom, dparams = list(mu = mean(arrests_n_per_day_nb$n), size=length(arrests_n_per_day_nb$n)),size=1, alpha=1, color="red")+ 
  labs(title="QQ Plot", subtitle="Poisson (in blue) and Negative Binomial (in red)", x="Theoretical Arrest Count", y="Sample Arrest Count")
```

```{r}
arrests_n_per_day_hist <- AnalysisDF %>% group_by(ArrestDay) %>% summarise(n = sum(Arrests))

raw_counts <- arrests_n_per_day_hist %>% group_by(n) %>% tally() %>% mutate(Distribution="Observed")
colnames(raw_counts) <- c("n","count","Distribution")

breaks_hist <- max(arrests_n_per_day_hist$n) - min(arrests_n_per_day_hist$n)

poisson_sample <- as.data.frame(rpois(length(arrests_n_per_day_hist$n), mean(arrests_n_per_day_hist$n)))
colnames(poisson_sample) <- c("sample_poisson")

poisson_sample <- poisson_sample <- poisson_sample %>% group_by(sample_poisson) %>% tally() %>% mutate(Distribution="Poisson (Sample)")
colnames(poisson_sample) <- c("n","count","Distribution")

nb_sample <- as.data.frame(rnegbin(length(arrests_n_per_day_hist$n), mu=mean(arrests_n_per_day_hist$n), theta.ml(arrests_n_per_day_hist$n, mu=mean(arrests_n_per_day_hist$n, n=length(arrests_n_per_day_hist$n)))))
colnames(nb_sample) <- c("sample_nb")

nb_sample <- nb_sample %>% group_by(sample_nb) %>% tally() %>% mutate(Distribution="Negative Binomial (Sample)") 
colnames(nb_sample) <- c("n","count","Distribution")

dist_df <- as.data.frame(rbind(raw_counts, poisson_sample, nb_sample))
dist_df$Distribution <- factor(dist_df$Distribution, levels = c("Observed", "Poisson (Sample)", "Negative Binomial (Sample)"), ordered = TRUE)

ggplot(data=dist_df,aes(x=n,y=count, color=Distribution)) +
  geom_line(size = 0.5) + 
  scale_color_manual(values=c("black", "blue", "red")) + 
  ggtitle("Distribution of Arrests Counts per Day") + 
  xlab("Arrests") +
  ylab("Days") +
  theme(legend.position="bottom")

sum(arrests_n_per_day_hist$n)
length(BPD_Arrests_March1$ArrestDate)
```

# Data analysis 

```{r}
colnames(AnalysisDF)
```

Let's create the linear spline for the approval of the consent decree.

**DOES THIS METHOD WORK?** [LINK TO METHOD](https://faculty.washington.edu/heagerty/Courses/b571/homework/spline-tutorial.q)

**WHY IS AIC NOT PRINTING PROPERLY?**
```{r}
AnalysisDF$CD_lin_spline <- (AnalysisDF$ArrestDate - ymd("2017-04-07"))
AnalysisDF$CD_lin_spline[AnalysisDF$CD_lin_spline<0] <- 0
```

How does our temporal data look like? 

Our temporal data is currently a date class, meaning R will recognize it as numeric. This will pose problems, as our model will resultantly try to fit a linear model and thus not account for trends within a certain time unit but attempt to explain an average change per time unit. In order to account for trends, we must create indicator variables for our temporal data. 

`lubridate` has excellent functions for this. A good rule of thumb for properly creating indicator variables from date classes is that the output of a function that extracts values from date classes has to be ready for interpretable categorization with `as.factor`. In other words, the unique values produced during extraction should correspond to the intended categories. 

```{r}
AnalysisDF$ArrestWeek_IV <- as.factor(week(AnalysisDF$ArrestDate))
AnalysisDF$ArrestMonth_IV <- as.factor(month(AnalysisDF$ArrestDate))
AnalysisDF$ArrestQuarter_IV <- as.factor(quarter(AnalysisDF$ArrestDate))
AnalysisDF$ArrestYear_IV <- as.factor(year(AnalysisDF$ArrestDate))
```

Seasons also need to be generated. A good thing to know is that winter starts in December. With that information and `floor_date`, which rounds date class data down to the lowest boundary, we can assign seasons to our temporal data.

```{r}
AnalysisDF$ArrestSeason_IV <- floor_date(AnalysisDF$ArrestDate, "season")
AnalysisDF$ArrestSeason_IV <- months(AnalysisDF$ArrestSeason_IV)
AnalysisDF$ArrestSeason_IV <- as.factor(AnalysisDF$ArrestSeason_IV)
AnalysisDF$ArrestSeason_IV <- factor(AnalysisDF$ArrestSeason_IV, levels = c("March", "June", "September","December"), labels = c("Spring","Summer","Fall","Winter"))
```

```{r}
colnames(AnalysisDF)
```

```{r}
MODELA1 <- glm(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareA = sum(residuals(MODELA1, type = "pearson")^2); degrees_freedomA = MODELA1$df.residual;
p_valueA = 1-pchisq(chi_squareA, degrees_freedomA)
paste0("Pearson goodness of fit = ", round(chi_squareA,2), ", df = ", degrees_freedomA, ", p-value = ", round(p_valueA,10))

MODELA2 <- glm.nb(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline, data=AnalysisDF, offset(log(Population)))

AIC_A <- AIC(MODELA2)
```

```{r}
MODELB1 <- glm(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareB = sum(residuals(MODELB1, type = "pearson")^2); degrees_freedomB = MODELB1$df.residual;
p_valueB = 1-pchisq(chi_squareB, degrees_freedomB)
paste0("Pearson goodness of fit = ", round(chi_squareB,2), ", df = ", degrees_freedomB, ", p-value = ", round(p_valueB,10))

MODELB2 <- glm.nb(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday, data=AnalysisDF, offset(log(Population)))

AIC_B <- AIC(MODELB2)
```

```{r}
MODELC1 <- glm(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestQuarter_IV, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareC = sum(residuals(MODELC1, type = "pearson")^2); degrees_freedomC = MODELC1$df.residual;
p_valueC = 1-pchisq(chi_squareC, degrees_freedomC)
paste0("Pearson goodness of fit = ", round(chi_squareC,2), ", df = ", degrees_freedomC, ", p-value = ", round(p_valueC,10))

MODELC2 <- glm.nb(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestQuarter_IV, data=AnalysisDF, offset(log(Population)))

AIC_C <- AIC(MODELC2)
```

```{r}
MODELD1 <- glm(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestSeason_IV, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareD = sum(residuals(MODELD1, type = "pearson")^2); degrees_freedomD = MODELD1$df.residual;
p_valueD = 1-pchisq(chi_squareD, degrees_freedomD)
paste0("Pearson goodness of fit = ", round(chi_squareD,2), ", df = ", degrees_freedomD, ", p-value = ", round(p_valueD,10))

MODELD2 <- glm.nb(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestSeason_IV, data=AnalysisDF, offset(log(Population)))

AIC_D <- AIC(MODELD2)
```

It appears that our quarter indicator variable better explains within year variation than does that season indicator variable.

```{r}
MODELE1 <- glm(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestMonth_IV, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareE = sum(residuals(MODELE1, type = "pearson")^2); degrees_freedomE = MODELE1$df.residual;
p_valueE = 1-pchisq(chi_squareE, degrees_freedomE)
paste0("Pearson goodness of fit = ", round(chi_squareE,2), ", df = ", degrees_freedomE, ", p-value = ", round(p_valueE,10))

MODELE2 <- glm.nb(Arrests ~ Gender + AgeGroup + Race + ArrestDay + CD_lin_spline + ArrestWeekday + ArrestMonth_IV, data=AnalysisDF, offset(log(Population)))

AIC_E <- AIC(MODELE2)
```

It appears that our month indicator variable explains within year variation moreso than our quarter indicator variable.

We will create one last model that will determine whether there is an interaction between our consent decree and race indicator variables.

```{r}
MODELF1 <- glm(Arrests ~ Gender + AgeGroup + ArrestDay + ArrestWeekday + ArrestMonth_IV + Race * CD_lin_spline, data=AnalysisDF, family=poisson, offset(log(Population)))

chi_squareF = sum(residuals(MODELF1, type = "pearson")^2); degrees_freedomF = MODELF1$df.residual;
p_valueF = 1-pchisq(chi_squareF, degrees_freedomF)
paste0("Pearson goodness of fit = ", round(chi_squareF,2), ", df = ", degrees_freedomF, ", p-value = ", round(p_valueF,10))

MODELF2 <- glm.nb(Arrests ~ Gender + AgeGroup + ArrestDay + ArrestWeekday + ArrestMonth_IV + Race * CD_lin_spline, data=AnalysisDF, offset(log(Population)))

AIC_F <- AIC(MODELF2)
```

It seems that we have ran out of covariates to explore. 

Let us review the AIC values for our models. 

```{r}
model_names <- c("A",
                 "B",
                 "C",
                 "D",
                 "E",
                 "F")

model_AICs <- c(AIC_A,
                AIC_B,
                AIC_C,
                AIC_D,
                AIC_E,
                AIC_F)

model_AICs <- round(model_AICs, 0)

model_AICs_table<- cbind(model_names, model_AICs)

colnames(model_AICs_table) <- c("Model","AIC")

kable(model_AICs_table) %>%
  kable_styling(bootstrap_options = c("bordered","condensed")) %>%
  row_spec(6, bold = T, color = "white", background = "black")
```

`r round((exp(summary(MODELF2)$coefficients["RaceWhite:CD_lin_spline","Estimate"])*1000)-1000,2)` more white people were arrested per 1000 arrests made. 

# Summary of results

The consent decree was followed by an increase in (approximately `r round((exp(summary(MODELF2)$coefficients["CD_lin_spline","Estimate"])*1000)-1000,2)` more arrests per 1000 arrests made prior to the consent decree. Despite this increase, people who were Black were less susceptible to arrest following the implementation of the consent decree. Following the implementation of the consent decree, there were approximately *Y* less arrests of Black people per 1000 arrests made prior to its implementation. 

Ultimately, the consent decree led to an overall increase in arrest rates for Black and White people in Baltimore. The consent decree did, however, reduce the disparities in arrest rates between Black and White people in Baltimore.